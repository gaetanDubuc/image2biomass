# Guide de Workflow Complet : DINOv2 ‚Üí XGBoost Optimis√©

Ce guide pr√©sente le workflow complet pour entra√Æner et optimiser un mod√®le XGBoost sur les features DINOv2.

## Vue d'ensemble

```
Images ‚Üí DINOv2 ‚Üí Features CSV ‚Üí XGBoost ‚Üí Pr√©dictions
                     ‚Üì
                  Optuna tuning ‚Üí Meilleurs hyperparam√®tres
```

## √âtape 1 : Pr√©paration des donn√©es

### 1.1 T√©l√©charger les donn√©es

```bash
make ingest
```

Cela t√©l√©charge les images de la comp√©tition Kaggle CSIRO Biomass dans `data/raw/`.

### 1.2 Cr√©er les splits (train/val/test)

```bash
make data
```

Cela cr√©e les splits dans `data/processed/splits/` par diff√©rentes strat√©gies :
- `month/` : splits par mois
- `Species/` : splits par esp√®ce
- `State/` : splits par √©tat

Chaque split contient des sous-dossiers num√©rot√©s (0/, 1/, 2/...) pour la cross-validation.

## √âtape 2 : Extraction des features DINOv2

### 2.1 Extraire les features (mod√®le base recommand√©)

```bash
# DINOv2 base (meilleure qualit√©, plus lent)
uv run image2biomass/features.py \
  data/raw/train \
  --output data/processed/features_dinov2_base.csv \
  --model facebook/dinov2-base \
  --batch-size 16

# OU DINOv2 small (plus rapide, qualit√© correcte)
uv run image2biomass/features.py \
  data/raw/train \
  --output data/processed/features_dinov2_small.csv \
  --model facebook/dinov2-small \
  --batch-size 32
```

**Temps estim√©** : 
- DINOv2-small : ~30-60 min sur GPU (M1/M2 Mac ou CUDA)
- DINOv2-base : ~1-2h sur GPU

Le CSV r√©sultant contient :
- Colonne `image_path` : chemin relatif de l'image
- Colonnes `f_0000` √† `f_0767` (ou `f_0383` pour small) : embeddings

## √âtape 3 : Entra√Ænement baseline

### 3.1 Entra√Æner un mod√®le XGBoost avec les param√®tres par d√©faut

```bash
make train \
  MODEL_NAME=xgboost \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  LABELS_PATH=data/processed/splits/month/0/train_split.csv \
  MODEL_OUT=models/xgboost_baseline.pkl
```

### 3.2 √âvaluer le mod√®le baseline

```bash
make evaluate \
  MODEL_NAME=xgboost \
  MODEL_PATH=models/xgboost_baseline.pkl \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  LABELS_PATH=data/processed/splits/month/0/val_split.csv
```

Notez le score R¬≤ de r√©f√©rence (baseline).

## √âtape 4 : Optimisation des hyperparam√®tres

### 4.1 Lancer l'optimisation Optuna (recommand√© : 100-200 trials)

```bash
make tune \
  TRAIN_SPLIT=data/processed/splits/month/0/train_split.csv \
  VAL_SPLIT=data/processed/splits/month/0/val_split.csv \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  N_TRIALS=150
```

**Temps estim√©** : 1-3h selon les donn√©es et le mat√©riel

### 4.2 Analyser les r√©sultats

```python
import pandas as pd
import matplotlib.pyplot as plt

# Charger l'historique
df = pd.read_csv("models/xgboost_tuned_optuna_study.csv")

# Top 10 essais
print(df.nsmallest(10, "value")[["number", "value", 
                                   "params_eta", 
                                   "params_max_depth",
                                   "params_n_estimators"]])

# Visualiser l'√©volution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(df["number"], -df["value"])  # Positive R¬≤
plt.xlabel("Trial")
plt.ylabel("R¬≤ score")
plt.title("Optimization Progress")

plt.subplot(1, 2, 2)
plt.scatter(df["params_eta"], -df["value"], alpha=0.5)
plt.xlabel("Learning Rate (eta)")
plt.ylabel("R¬≤ score")
plt.xscale("log")
plt.title("Learning Rate vs Performance")

plt.tight_layout()
plt.savefig("reports/figures/tuning_analysis.png")
```

### 4.3 Comparer avec le baseline

```bash
make evaluate \
  MODEL_NAME=xgboost \
  MODEL_PATH=models/xgboost_tuned.pkl \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  LABELS_PATH=data/processed/splits/month/0/val_split.csv
```

**Am√©lioration attendue** : +2% √† +10% de R¬≤ selon les donn√©es

## √âtape 5 : Int√©grer les meilleurs hyperparam√®tres

### 5.1 Mettre √† jour la Config dans `xgboost_regressor.py`

Ouvrir `image2biomass/modeling/models/xgboost_regressor.py` et modifier la classe `Config` :

```python
@dataclass(frozen=True)
class Config:
    # Depuis les r√©sultats Optuna (exemple)
    n_estimators: int = 250      # √©tait 100
    learning_rate: float = 0.05  # √©tait 0.1
    max_depth: int = 7           # √©tait 3
    min_child_weight: int = 3    # √©tait 1
    gamma: float = 0.2           # √©tait 0
    subsample: float = 0.85      # √©tait 1.0
    colsample_bytree: float = 0.9  # √©tait 1.0
    objective: str = "reg:squarederror"
    n_jobs: int = -1
    random_state: int = 42
```

## √âtape 6 : Cross-validation compl√®te

### 6.1 Entra√Æner sur tous les folds

```bash
make cross-validate \
  MODEL_NAME=xgboost \
  SPLITS_DIR=data/processed/splits/month \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  --train
```

Cela va :
1. Entra√Æner un mod√®le par fold (`xgboost_fold0.pkl`, `xgboost_fold1.pkl`, ...)
2. √âvaluer chaque mod√®le sur son fold de validation
3. Afficher les scores moyens et √©cart-type

**R√©sultat attendu** :
```
Cross-validation complete!
Scores per fold: ['0.7234', '0.7189', '0.7301', '0.7156', '0.7267']
Mean R¬≤: 0.7229 ¬± 0.0052
```

### 6.2 Analyser la stabilit√©

Si l'√©cart-type est √©lev√© (>0.02), cela peut indiquer :
- **Overfitting** : r√©duire `max_depth` ou `n_estimators`
- **Underfitting** : augmenter `n_estimators` ou `learning_rate`
- **Distribution d√©s√©quilibr√©e** : v√©rifier les splits

## √âtape 7 : Pr√©dictions finales

### 7.1 Choisir le meilleur fold ou faire un ensemble

```bash
# Option 1 : utiliser le meilleur mod√®le d'un fold
make evaluate \
  MODEL_NAME=xgboost \
  MODEL_PATH=models/xgboost_fold2.pkl \
  FEATURES_PATH=data/processed/features_dinov2_base.csv \
  LABELS_PATH=data/raw/test.csv

# Option 2 : moyenner les pr√©dictions de tous les folds (ensemble)
# (n√©cessite un script custom)
```

### 7.2 Soumettre √† Kaggle

```bash
# G√©n√©rer le fichier de soumission
uv run image2biomass/modeling/predict.py \
  --model-path models/xgboost_fold2.pkl \
  --features-path data/processed/features_dinov2_base.csv \
  --output data/processed/submission.csv

# Soumettre
kaggle competitions submit -c csiro-biomass \
  -f data/processed/submission.csv \
  -m "XGBoost with optimized hyperparameters on DINOv2-base features"
```

## Conseils avanc√©s

### Am√©liorer les performances

1. **Essayer d'autres strat√©gies de split**
   ```bash
   make cross-validate SPLITS_DIR=data/processed/splits/Species
   ```

2. **Combiner plusieurs extractions de features**
   ```python
   # Fusionner dinov2_small et dinov2_base
   import polars as pl
   
   df_small = pl.read_csv("data/processed/features_dinov2_small.csv")
   df_base = pl.read_csv("data/processed/features_dinov2_base.csv")
   
   # Renommer les colonnes pour √©viter les conflits
   df_small = df_small.rename({f"f_{i:04d}": f"small_{i:04d}" 
                                for i in range(384)})
   
   df_combined = df_small.join(df_base, on="image_path")
   df_combined.write_csv("data/processed/features_combined.csv")
   ```

3. **Tuner avec plusieurs folds**
   ```bash
   # Tuner sur fold 0
   make tune TRAIN_SPLIT=data/processed/splits/month/0/train_split.csv \
             VAL_SPLIT=data/processed/splits/month/0/val_split.csv \
             N_TRIALS=100
   
   # Valider sur fold 1
   make tune TRAIN_SPLIT=data/processed/splits/month/1/train_split.csv \
             VAL_SPLIT=data/processed/splits/month/1/val_split.csv \
             N_TRIALS=50
   ```

4. **Utiliser un ensemble de mod√®les**
   - Moyenne pond√©r√©e des pr√©dictions de tous les folds
   - Stacking avec un meta-mod√®le (LightGBM, CatBoost)

### D√©boguer les probl√®mes

**Probl√®me : R¬≤ n√©gatif ou tr√®s faible**
- V√©rifier que les features et labels sont bien align√©s (m√™me `image_path`)
- V√©rifier la distribution des targets (y) : doivent √™tre continues, pas de valeurs manquantes
- Essayer avec un mod√®le plus simple (sklearn RandomForest)

**Probl√®me : Temps d'entra√Ænement trop long**
- R√©duire `n_estimators` temporairement
- Utiliser `n_jobs=-1` pour parall√©liser
- Commencer avec moins de trials Optuna (50 au lieu de 150)

**Probl√®me : Optuna plante avec OOM**
- R√©duire `batch_size` dans les features
- Limiter les donn√©es avec un subset pour le tuning
- Utiliser des machines avec plus de RAM

## Checklist compl√®te

- [ ] Donn√©es t√©l√©charg√©es (`make ingest`)
- [ ] Splits cr√©√©s (`make data`)
- [ ] Features DINOv2 extraites
- [ ] Mod√®le baseline entra√Æn√© et √©valu√©
- [ ] Hyperparam√®tres optimis√©s avec Optuna (100+ trials)
- [ ] Config mise √† jour avec les meilleurs param√®tres
- [ ] Cross-validation compl√®te ex√©cut√©e
- [ ] Scores analys√©s et stables (std < 0.02)
- [ ] Pr√©dictions g√©n√©r√©es et soumises

**Bon entra√Ænement ! üöÄ**
